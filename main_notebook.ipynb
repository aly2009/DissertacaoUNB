{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5370e09-4acf-40d3-b705-2951a32e2604",
   "metadata": {},
   "source": [
    "<h1>Log Parsing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09938999-e0d3-4b31-b468-37a5bd1d8293",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import log_parsing\n",
    "import data_preprocessing\n",
    "import evaluation_metrics\n",
    "\n",
    "log_file_path = 'C:/Users/Alysson/Desktop/DissertacaoUNB/datasets/BGL/BGL_2k.log'\n",
    "structured_file_path = 'C:/Users/Alysson/Desktop/DissertacaoUNB/datasets/BGL/BGL_2k.log_structured.csv'\n",
    "parser = 'Drain'\n",
    "log_format = '<Label> <Timestamp> <Date> <Node> <Time> <NodeRepeat> <Type> <Component> <Level> <Content>'\n",
    "rex  = [\n",
    "        r'core\\.\\d+',  # Correspondente ao padrão 'core.' seguido de um ou mais dígitos.\n",
    "        r'blk_(|-)[0-9]+',  # Correspondente ao padrão 'blk_' seguido opcionalmente por '-' e um ou mais dígitos (bloco ID).\n",
    "        r'(/|)([0-9]+\\.){3}[0-9]+(:[0-9]+|)(:|)',  # Correspondente a endereços IP no formato x.x.x.x:porta ou x.x.x.x.\n",
    "        r'([0-9a-f]+[:][0-9a-f]+)',  # Correspondente a pares hexadecimal no formato 0x0:0x0.\n",
    "        r'fpr[0-9]+[=]0x[0-9a-f]+ [0-9a-f]+ [0-9a-f]+ [0-9a-f]+',  # Correspondente a padrões 'fprX=0xX X X X'.\n",
    "        r'r[0-9]+[=]0x[0-9a-f]+',  # Correspondente a padrões 'rX=0xX'.\n",
    "        r'[l|c|xe|ct]r=0x[0-9a-f]+',  # Correspondente a padrões como 'lr=0xX' ou 'cr=0xX'.\n",
    "        r'0x[0-9a-f]+',  # Correspondente a valores hexadecimais no formato 0xX.\n",
    "        r'(?<=[^A-Za-z0-9])(\\-?\\+?\\d+)(?=[^A-Za-z0-9])|[0-9]+$',  # Correspondente a números inteiros ou números separados por não caracteres alfanuméricos.\n",
    "        ]\n",
    "\n",
    "outdir = 'datasets/BGL/'\n",
    "\n",
    "# Chama o parsing de logs\n",
    "structured_log_df = log_parsing.process_log_files(structured_file_path, log_file_path, parser, log_format, outdir, rex, config_file_path)\n",
    "\n",
    "structured_log_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae5fa2d-cb2e-4a3a-83c2-7da3290cf0af",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1>Cria log sequences</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97310e3f-5997-44a7-aec8-919234956136",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Adicione o caminho do módulo ao sys.path\n",
    "module_path = r'C:\\Users\\Alysson\\Desktop\\DissertacaoUNB'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from log_parsing import load_BGL_data\n",
    "from data_preprocessing import sample_data\n",
    "\n",
    "\n",
    "def create_sequences(nome_arquivo, Label=True):\n",
    "    nome_do_arquivo_aux = os.path.basename(nome_arquivo)\n",
    "    nome_do_arquivo_sem_extensao = os.path.splitext(nome_do_arquivo_aux)[0]\n",
    "    diretorio_do_arquivo = os.path.dirname(nome_arquivo)\n",
    "\n",
    "    # Remova a extensão .log_structured do nome do arquivo\n",
    "    nome_do_arquivo_sem_extensao = nome_do_arquivo_sem_extensao.replace(\".log_structured\", \"\")\n",
    "\n",
    "    # Caminho para o diretório 'logsequences'\n",
    "    logsequences_dir = os.path.join(diretorio_do_arquivo, \"logsequences\")\n",
    "\n",
    "    # Verifique se o diretório 'logsequences' existe, senão, crie-o\n",
    "    if not os.path.exists(logsequences_dir):\n",
    "        os.makedirs(logsequences_dir)\n",
    "\n",
    "    config = {\n",
    "        \"window_size\": 0.01667,\n",
    "        \"step_size\": 0.01667,\n",
    "        \"structured_file_path\": os.path.join(diretorio_do_arquivo, f\"{nome_do_arquivo_sem_extensao}.log_structured.csv\"),\n",
    "        \"sequence_path\": os.path.join(logsequences_dir, f\"{nome_do_arquivo_sem_extensao}.csv\")\n",
    "    }\n",
    "\n",
    "    BGL_structured_data = load_BGL_data(config[\"structured_file_path\"], create_seconds_since=True, Label=Label)\n",
    "    sequenceVector = sample_data(BGL_structured_data, config[\"window_size\"], config[\"step_size\"], config[\"sequence_path\"], Label=Label, window_type='time')\n",
    "\n",
    "    print(f\"Executando create_sequences para {nome_arquivo}\")\n",
    "    \n",
    "def processar_arquivos_csv_em_diretorio(diretorio, Label=False):\n",
    "    # Verifique se o diretório existe\n",
    "    if os.path.exists(diretorio) and os.path.isdir(diretorio):\n",
    "        # Liste os arquivos no diretório\n",
    "        arquivos = os.listdir(diretorio)\n",
    "\n",
    "        for arquivo in arquivos:\n",
    "            if arquivo.endswith(\".log_structured.csv\"):\n",
    "                create_sequences(os.path.join(diretorio, arquivo), Label=Label)\n",
    "    else:\n",
    "        print(f\"O diretório '{diretorio}' não existe.\")\n",
    "\n",
    "# Diretório 1\n",
    "diretorio1 = r'C:\\Users\\Alysson\\Desktop\\DissertacaoUNB\\datasets\\BGL'\n",
    "processar_arquivos_csv_em_diretorio(diretorio1, Label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339c9fad-301e-44ca-a0d1-9b93ce0a1dff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Caminho do módulo ao sys.path\n",
    "module_path = r'C:\\Users\\Alysson\\Desktop\\DissertacaoUNB'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from create_train_test_dataset import create_train_test_dataset\n",
    "from semantic_vectorization import generate_word_vectors_dict, process_line, map_log_sequence_to_word_vec, create_tfidf_matrix_log_sequence, create_weight_log_sequence, create_semantic_log_seq, create_semantic_vector_sequence\n",
    "\n",
    "########## PROCESSAMENTO DAS LOG SEQUENCES BGL ########## \n",
    "templatesTrain = pd.read_csv(r'C:\\Users\\Alysson\\Desktop\\DissertacaoUNB\\datasets\\BGL\\BGL_2k.log_templates.csv', sep=',')\n",
    "SequenceVecTrain = pd.read_csv(r'C:\\Users\\Alysson\\Desktop\\DissertacaoUNB\\datasets\\BGL\\logsequences\\BGL_2k.csv', sep=',')\n",
    "#SequenceVecTest = pd.read_csv(r'C:\\Users\\Alysson\\Desktop\\DissertacaoUNB\\datasets\\BGL\\XXXXXXX.csv', sep=',') #Opcional\n",
    "#templatesTest = pd.read_csv(r'C:\\Users\\Alysson\\Desktop\\DissertacaoUNB\\datasets\\BGL\\XXXXXXX.csv', sep=',') #Opcional\n",
    "preProcess = {\"stemming\": False, \"lemmatization\": True}\n",
    "\n",
    "train_data, test_data, _ = create_train_test_dataset(templatesTrain, SequenceVecTrain, onlyNormalDataTrain=True, shuffleTrain=True, removeEmptySeq=True, \n",
    "                                                  splitData=True, SequenceVectorTest=None, templatesTest=None, preProcessParam=preProcess)\n",
    "\n",
    "########## CRIAÇÃO DAS SEQUÊNCIAS DE VETORES SEMÂNTICOS - ÚNICA ########## \n",
    "\n",
    "# Use a função apply para aplicar a função process_line a cada linha da coluna 'ReplacedSequence'\n",
    "train_data['splittedReplacedSequence'] = train_data['ReplacedSequence'].apply(process_line)\n",
    "test_data['splittedReplacedSequence'] = test_data['ReplacedSequence'].apply(process_line) \n",
    "\n",
    "word_vector_dict_train, dimension_train = generate_word_vectors_dict(train_data, model_type='glove50d') \n",
    "#word_vector_dict_test, dimension_test = generate_word_vectors_dict(test_data, model_type='glove50d')\n",
    "\n",
    "train_data = create_semantic_vector_sequence(train_data, word_vector_dict_train, dimension_train)\n",
    "test_data = create_semantic_vector_sequence(test_data, word_vector_dict_train, dimension_train) \n",
    "\n",
    "print('Total train sequences: ' ,len(train_data))\n",
    "print('Total test sequences: ' ,len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b0cacf-71e8-4b56-97fe-ad9c0d153c05",
   "metadata": {},
   "source": [
    "<h1>Experimento 0</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72422ffa-3131-4db5-ae4a-628073078235",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# Adicione o caminho do módulo ao sys.path\n",
    "module_path = r'C:\\Users\\Alysson\\Desktop\\DissertacaoUNB'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from create_train_test_dataset import create_train_test_dataset\n",
    "from semantic_vectorization import *\n",
    "from anomaly_detection import *\n",
    "\n",
    "########## PROCESSAMENTO LOG BGL ########## \n",
    "templatesTrain = pd.read_csv(r'C:\\Users\\Alysson\\Desktop\\DissertacaoUNB\\datasets\\BGL\\BGL_2k.log_templates.csv', sep=',')\n",
    "SequenceVecTrain = pd.read_csv(r'C:\\Users\\Alysson\\Desktop\\DissertacaoUNB\\datasets\\BGL\\logsequences\\BGL_2k.csv', sep=',')\n",
    "#SequenceVecTest = pd.read_csv(r'C:\\Users\\Alysson\\Desktop\\MinhaImplementacaoBGL\\datasets\\BGL\\XXXXXXX.csv', sep=',') #Opcional\n",
    "#templatesTest = pd.read_csv(r'C:\\Users\\Alysson\\Desktop\\MinhaImplementacaoBGL\\datasets\\BGL\\XXXXXXX.csv', sep=',') #Opcional\n",
    "preProcess = {\"stemming\": False, \"lemmatization\": True}\n",
    "\n",
    "train_data, test_data, _ = create_train_test_dataset(templatesTrain, SequenceVecTrain, onlyNormalDataTrain=True, shuffleTrain=True, removeEmptySeq=True, \n",
    "                                                  splitData=True, SequenceVectorTest=None, templatesTest=None, preProcessParam=preProcess)\n",
    "\n",
    "########## CRIAÇÃO DAS SEQUÊNCIAS DE VETORES SEMÂNTICOS - ÚNICA ########## \n",
    "\n",
    "# Use a função apply para aplicar a função process_line a cada linha da coluna 'ReplacedSequence'\n",
    "train_data['splittedReplacedSequence'] = train_data['ReplacedSequence'].apply(process_line)\n",
    "test_data['splittedReplacedSequence'] = test_data['ReplacedSequence'].apply(process_line) \n",
    "\n",
    "word_vector_dict_train, dimension_train = generate_word_vectors_dict(train_data, model_type='glove50d') \n",
    "#word_vector_dict_test, dimension_test = generate_word_vectors_dict(test_data, model_type='glove50d')\n",
    "\n",
    "train_data = create_semantic_vector_sequence(train_data, word_vector_dict_train, dimension_train)\n",
    "test_data = create_semantic_vector_sequence(test_data, word_vector_dict_train, dimension_train) \n",
    "\n",
    "print('Total train sequences: ' ,len(train_data))\n",
    "print('Total test sequences: ' ,len(test_data))\n",
    "\n",
    "########## TREINAMENTO E TESTE DOS MODELOS ########## \n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "\n",
    "contamination = 0.1\n",
    "# LOF\n",
    "print('*'*50, ' LOF ', '*'*50)\n",
    "params = {'algorithm': 'auto', 'contamination': contamination, 'leaf_size': 30, 'metric': 'minkowski', 'n_jobs': None, 'n_neighbors': 20, 'novelty': True} # Default\n",
    "\n",
    "_, lof_df_anomalies, lof_df_prediction, lof_metricas, lof_total_time = detect_lof_anomalies(train_data, test_data, metric_params=params, label=True, \n",
    "                                                                                            plot_tsne=True, gridSearch=False)\n",
    "precision_lof, recall_lof, f1_lof, fpr_lof, tpr_lof, roc_auc_lof = lof_metricas\n",
    "train_time_lof, prediction_time_lof = lof_total_time\n",
    "\n",
    "# Exibe as primeiras sequências mais anômalas \n",
    "lof_df_anomalies[lof_df_anomalies['Anomaly_Prediction'] == -1].head(5)\n",
    "\n",
    "# Autoencoder\n",
    "print('*'*50, ' Autoencoder ', '*'*50)\n",
    "params = {'contamination':contamination}\n",
    "_, autoencoder_df_anomalies, _, autoencoder_metricas, autoencoder_total_time  = detect_autoencoder_anomalies(train_data, test_data, metric_params=params, label=True, plot_tsne=True)\n",
    "\n",
    "precision_autoencoder, recall_autoencoder, f1_autoencoder, fpr_autoencoder, tpr_autoencoder, roc_auc_autoencoder = autoencoder_metricas\n",
    "train_time_autoencoder, prediction_time_autoencoder = autoencoder_total_time\n",
    "\n",
    "# Exibe as primeiras sequências mais anômalas \n",
    "autoencoder_df_anomalies[autoencoder_df_anomalies['Anomaly_Prediction'] == -1].head(5)\n",
    "\n",
    "# SOM\n",
    "print('*'*50, ' SOM ', '*'*50)\n",
    "params = {'contamination':contamination, 'grid_size':(10, 10), 'sigma':1.0, 'learning_rate':0.5}\n",
    "_, som_df_anomalies, _, som_metricas, som_total_time  = detect_som_anomalies(train_data, test_data, metric_params=params, label=True, plot_tsne=True)\n",
    "\n",
    "precision_som, recall_som, f1_som, fpr_som, tpr_som, roc_auc_som = som_metricas\n",
    "train_time_som, prediction_time_som = som_total_time\n",
    "\n",
    "# Exibe as primeiras sequências mais anômalas \n",
    "som_df_anomalies[som_df_anomalies['Anomaly_Flag'] == -1].head(5)\n",
    "#export_anomalies_to_csv(som_df_anomalies, 'som_df_anomalies', r'C:\\Users\\Alysson\\Desktop\\MinhaImplementacaoBGL\\datasets\\Ccmeval\\anomaliasEncontradas')\n",
    "\n",
    "# Isolation Forest\n",
    "print('*'*50, ' IF ', '*'*50)\n",
    "params = {'contamination': contamination, 'max_features': 1.0, 'max_samples': 'auto', 'n_estimators': 100, 'n_jobs': None, 'random_state': 123} # Default\n",
    "_, isolation_forest_df_anomalies, isolation_forest_prediction, isolation_forest_metricas, isolation_forest_total_time = detect_isolation_forest_anomalies(train_data, test_data,\n",
    "                                                                                          metric_params=params, label=True, plot_tsne=True, gridSearch=False)\n",
    "\n",
    "precision_isolation_forest, recall_isolation_forest, f1_isolation_forest, fpr_isolation_forest, tpr_isolation_forest, roc_auc_isolation_forest = isolation_forest_metricas\n",
    "train_time_isolation_forest, prediction_time_isolation_forest = isolation_forest_total_time\n",
    "\n",
    "# Exibe as primeiras sequências mais anômalas \n",
    "isolation_forest_df_anomalies[isolation_forest_df_anomalies['Anomaly_Prediction'] == -1].head(5)\n",
    "#export_anomalies_to_csv(isolation_forest_df_anomalies, 'isolation_forest_df_anomalies', r'C:\\Users\\Alysson\\Desktop\\MinhaImplementacaoBGL\\datasets\\Ccmeval\\anomaliasEncontradas')\n",
    "\n",
    "# MCD\n",
    "print('*'*50, ' MCD ', '*'*50)\n",
    "params = {'contamination':contamination, 'support_fraction': 1} # Default ('support_fraction': 1 alterado para evitar os warnings)\n",
    "_, mcd_df_anomalies, mcd_prediction, mcd_metricas, mcd_total_time = detect_mcd_anomalies(train_data, test_data, metric_params=params, label=True, plot_tsne=True)\n",
    "\n",
    "precision_mcd, recall_mcd, f1_mcd, fpr_mcd, tpr_mcd, roc_auc_mcd = mcd_metricas\n",
    "train_time_mcd, prediction_time_mcd = mcd_total_time\n",
    "\n",
    "# Exibe as primeiras sequências mais anômalas \n",
    "mcd_df_anomalies[mcd_df_anomalies['Anomaly_Prediction'] == -1].head(5)\n",
    "\n",
    "# HBOS\n",
    "print('*'*50, ' HBOS ', '*'*50)\n",
    "params = {'n_bins':10, 'alpha':0.1, 'tol':0.5, 'contamination':contamination} # Default\n",
    "_, hbos_df_anomalies, hbos_prediction, hbos_metricas, hbos_total_time = detect_hbos_anomalies(train_data, test_data,\n",
    "                                                                                             metric_params=params, label=True, plot_tsne=True)\n",
    "\n",
    "precision_hbos, recall_hbos, f1_hbos, fpr_hbos, tpr_hbos, roc_auc_hbos = hbos_metricas\n",
    "train_time_hbos, prediction_time_hbos = hbos_total_time\n",
    "\n",
    "# Exibe as primeiras sequências mais anômalas\n",
    "hbos_df_anomalies[hbos_df_anomalies['Anomaly_Prediction'] == -1].head(5)  # Mostra as anomalias (classe -1)\n",
    "\n",
    "# CBLOF\n",
    "print('*'*50, ' CBLOF ', '*'*50)\n",
    "params= {'contamination':contamination } # Default\n",
    "_, cblof_df_anomalies, cblof_prediction, cblof_metricas, cblof_total_time = detect_cblof_anomalies(train_data, test_data,\n",
    "                                                                                             metric_params=params, label=True, plot_tsne=True)\n",
    "\n",
    "precision_cblof, recall_cblof, f1_cblof, fpr_cblof, tpr_cblof, roc_auc_cblof = cblof_metricas\n",
    "train_time_cblof, prediction_time_cblof = cblof_total_time\n",
    "\n",
    "# Exibe as primeiras sequências mais anômalas\n",
    "cblof_df_anomalies[cblof_df_anomalies['Anomaly_Prediction'] == -1].head(5)  # Mostra as anomalias (classe -1)\n",
    "\n",
    "# GMM\n",
    "print('*'*50, ' GMM ', '*'*50)\n",
    "params = {'contamination': contamination, 'n_components': 1, 'covariance_type': 'full', 'max_iter': 100, 'init_params': 'kmeans', 'random_state': 123} # Default\n",
    "_, gmm_df_anomalies, gmm_prediction, gmm_metricas, gmm_total_time = detect_gmm_anomalies(train_data, test_data,\n",
    "                                                                                         metric_params=params, label=True, plot_tsne=True)\n",
    "\n",
    "precision_gmm, recall_gmm, f1_gmm, fpr_gmm, tpr_gmm, roc_auc_gmm = gmm_metricas\n",
    "train_time_gmm, prediction_time_gmm = gmm_total_time\n",
    "\n",
    "# Exibe as primeiras sequências mais anômalas\n",
    "gmm_df_anomalies[gmm_df_anomalies['Anomaly_Prediction'] == -1].head(5)  # Mostra as anomalias (classe -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93a4403-f7d2-41d1-bad8-1e643bb8a106",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "################## Experimento 0 - Resultados ##################\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "models = ['LOF', 'AE', 'SOM', 'IF', 'MCD', 'HBOS', 'CBLOF', 'GMM']\n",
    "f1_scores = [f1_lof, f1_autoencoder, f1_som, f1_isolation_forest, f1_mcd, f1_hbos, f1_cblof, f1_gmm]\n",
    "recall_scores = [recall_lof, recall_autoencoder, recall_som, recall_isolation_forest, recall_mcd, recall_hbos, recall_cblof, recall_gmm]\n",
    "precision_scores = [precision_lof, precision_autoencoder, precision_som, precision_isolation_forest, precision_mcd, precision_hbos, precision_cblof, precision_gmm]\n",
    "roc_auc_scores = [roc_auc_lof, roc_auc_autoencoder, roc_auc_som, roc_auc_isolation_forest, roc_auc_mcd, roc_auc_hbos, roc_auc_cblof, roc_auc_gmm]\n",
    "\n",
    "# Valores de FPR, TPR e ROC AUC para cada modelo\n",
    "fpr_models = [fpr_lof, fpr_autoencoder, fpr_som, fpr_isolation_forest, fpr_mcd, fpr_hbos, fpr_cblof, fpr_gmm]\n",
    "tpr_models = [tpr_lof, tpr_autoencoder, tpr_som, tpr_isolation_forest, tpr_mcd, tpr_hbos, tpr_cblof, tpr_gmm]\n",
    "roc_auc_models = [roc_auc_lof, roc_auc_autoencoder, roc_auc_som, roc_auc_isolation_forest, roc_auc_mcd, roc_auc_hbos, roc_auc_cblof, roc_auc_gmm]\n",
    "\n",
    "# Tempos de treinamento e predição para cada modelo\n",
    "train_times = [train_time_lof, train_time_autoencoder, train_time_som, train_time_isolation_forest, train_time_mcd, train_time_hbos, train_time_cblof, train_time_gmm]\n",
    "prediction_times = [prediction_time_lof, prediction_time_autoencoder, prediction_time_som, prediction_time_isolation_forest, prediction_time_mcd, prediction_time_hbos, prediction_time_cblof, prediction_time_gmm]\n",
    "\n",
    "# Cálculo dos tempos totais (treinamento + predição)\n",
    "total_times = [train_time + prediction_time for train_time, prediction_time in zip(train_times, prediction_times)]\n",
    "\n",
    "# Configuração das posições das barras\n",
    "bar_width = 0.15\n",
    "index = np.arange(len(models))\n",
    "\n",
    "# Configuração dos eixos e legendas para as métricas e tempos\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 8))\n",
    "\n",
    "# Aumentar o espaçamento entre subplots\n",
    "plt.subplots_adjust(wspace=0.5, hspace=0.6)\n",
    "\n",
    "# Gráfico de barras das métricas\n",
    "axes[0, 0].bar(index, f1_scores, bar_width, label='F1-score', alpha=0.7)\n",
    "axes[0, 0].bar(index + bar_width, recall_scores, bar_width, label='Recall', alpha=0.7)\n",
    "axes[0, 0].bar(index + 2 * bar_width, precision_scores, bar_width, label='Precision', alpha=0.7)\n",
    "\n",
    "axes[0, 0].set_title('Comparação de Métricas de Desempenho', fontsize=12)\n",
    "axes[0, 0].set_xticks(index + 1.5 * bar_width)\n",
    "axes[0, 0].set_xticklabels(models, rotation=45)\n",
    "axes[0, 0].legend(loc='upper right', fontsize='small')\n",
    "axes[0, 0].grid(axis='y', linestyle='--', alpha=0.6)\n",
    "axes[0, 0].set_ylim(0.0, 1.1)  # Define o limite do eixo y\n",
    "\n",
    "# Gráfico de barras para o tempo de predição\n",
    "axes[1, 0].set_title('Tempo de Predição', fontsize=12)\n",
    "axes[1, 0].set_xticks(index + bar_width / 2)\n",
    "axes[1, 0].set_xticklabels(models, rotation=45)\n",
    "axes[1, 0].set_ylabel('Tempo (s)')\n",
    "\n",
    "axes[1, 0].bar(index, prediction_times, 3 * bar_width, label='Predição', alpha=0.7, color='lightcoral')\n",
    "\n",
    "# Gráfico de barras para o tempo de treinamento\n",
    "axes[1, 1].set_title('Tempo de Treinamento', fontsize=12)\n",
    "axes[1, 1].set_xticks(index + bar_width / 2)\n",
    "axes[1, 1].set_xticklabels(models, rotation=45)\n",
    "axes[1, 1].set_ylabel('Tempo (s)')\n",
    "\n",
    "axes[1, 1].bar(index, train_times, 3 * bar_width, label='Treinamento', alpha=0.7, color='skyblue')\n",
    "\n",
    "# Gráfico de barras para o tempo total (treinamento + predição)\n",
    "axes[1, 2].set_title('Tempo Total', fontsize=12)\n",
    "axes[1, 2].set_xticks(index + bar_width / 2)\n",
    "axes[1, 2].set_xticklabels(models, rotation=45)\n",
    "axes[1, 2].set_ylabel('Tempo (s)')\n",
    "\n",
    "axes[1, 2].bar(index, total_times, 3 * bar_width, label='Total', alpha=0.7, color='lightgreen')\n",
    "\n",
    "# Gráfico da curva ROC comparativa\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'purple', 'orange']  # Cores diferentes para cada modelo\n",
    "linestyles = ['-', '--', '-.', ':', '-', '--', '-.', ':', '-']\n",
    "line_width = 2.0  \n",
    "\n",
    "for i in range(len(models)):\n",
    "    axes[0, 2].plot(fpr_models[i], tpr_models[i], color=colors[i], linestyle=linestyles[i], lw=line_width,\n",
    "                 label=f'{models[i]} (AUC = {roc_auc_models[i]:.2f})')\n",
    "\n",
    "axes[0, 2].set_xlim([-0.03, 1.03])\n",
    "axes[0, 2].set_ylim([-0.05, 1.05])\n",
    "axes[0, 2].set_xlabel('Taxa de Falsos Positivos (FPR)')\n",
    "axes[0, 2].set_ylabel('Taxa de Verdadeiros Positivos (TPR)')\n",
    "axes[0, 2].set_title('Curva ROC Comparativa', fontsize=12)\n",
    "axes[0, 2].plot([0, 1], [0, 1], linestyle='-', color='red', linewidth=0.5, label='Classificador Aleatório')\n",
    "axes[0, 2].legend(loc=\"lower right\", fontsize='x-small')\n",
    "axes[0, 2].grid(linestyle='--', alpha=0.6)\n",
    "\n",
    "\n",
    "# Todos os tempos\n",
    "axes[0, 1].set_title('Tempos', fontsize=12)\n",
    "axes[0, 1].set_xticks(index + bar_width / 2)\n",
    "axes[0, 1].set_xticklabels(models, rotation=45)\n",
    "axes[0, 1].set_ylabel('Tempo (s)')\n",
    "\n",
    "# Gráfico de barras para o tempo de predição\n",
    "axes[0, 1].bar(index - bar_width, prediction_times, bar_width, label='Predição', alpha=0.7, color='lightcoral')\n",
    "# Gráfico de barras para o tempo de treinamento\n",
    "axes[0, 1].bar(index, train_times, bar_width, label='Treinamento', alpha=0.7, color='skyblue')\n",
    "# Gráfico de barras para o tempo total\n",
    "axes[0, 1].bar(index + bar_width, total_times, bar_width, label='Total', alpha=0.7, color='lightgreen')\n",
    "\n",
    "# Adicione uma legenda para o subplot dos tempos\n",
    "axes[0, 1].legend(loc='upper right', fontsize='small')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig('metricas_e_roc.png', dpi=300, bbox_inches='tight')  # Salvar a figura em alta resolução\n",
    "plt.show()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "models = ['LOF', 'AE', 'SOM', 'IF', 'MCD', 'HBOS', 'CBLOF', 'GMM']\n",
    "f1_scores = [f1_lof, f1_autoencoder, f1_som, f1_isolation_forest, f1_mcd, f1_hbos, f1_cblof, f1_gmm]\n",
    "recall_scores = [recall_lof, recall_autoencoder, recall_som, recall_isolation_forest, recall_mcd, recall_hbos, recall_cblof, recall_gmm]\n",
    "precision_scores = [precision_lof, precision_autoencoder, precision_som, precision_isolation_forest, precision_mcd, precision_hbos, precision_cblof, precision_gmm]\n",
    "\n",
    "# Configuração das posições das barras\n",
    "bar_width = 0.5\n",
    "index = np.arange(len(models))\n",
    "\n",
    "# Configuração dos eixos e legendas para as métricas\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))  # 1 linha e 3 colunas\n",
    "\n",
    "# Gráfico de barras do F1-score\n",
    "bar1 = axes[0].bar(index, f1_scores, bar_width, label='F1-score', alpha=0.7)\n",
    "axes[0].set_title('F1-Score', fontsize=12)\n",
    "axes[0].set_xticks(index)\n",
    "axes[0].set_xticklabels(models, rotation=45)\n",
    "axes[0].yaxis.grid(True, linestyle='--', alpha=0.6)\n",
    "axes[0].xaxis.grid(False)  # Desativar o grid vertical\n",
    "axes[0].set_ylim(0.0, 1.1)\n",
    "\n",
    "# Adicionar valores no topo das barras do F1-score\n",
    "for bar, score in zip(bar1, f1_scores):\n",
    "    axes[0].annotate(f'{score:.2f}', xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()), xytext=(0, 3),\n",
    "                    textcoords=\"offset points\", ha='center', va='bottom')\n",
    "\n",
    "# Gráfico de barras do Recall\n",
    "bar2 = axes[1].bar(index, recall_scores, bar_width, label='Recall', alpha=0.7, color='g')\n",
    "axes[1].set_title('Recall', fontsize=12)\n",
    "axes[1].set_xticks(index)\n",
    "axes[1].set_xticklabels(models, rotation=45)\n",
    "axes[1].yaxis.grid(True, linestyle='--', alpha=0.6)\n",
    "axes[1].xaxis.grid(False)  # Desativar o grid vertical\n",
    "axes[1].set_ylim(0.0, 1.1)\n",
    "\n",
    "# Adicionar valores no topo das barras do Recall\n",
    "for bar, score in zip(bar2, recall_scores):\n",
    "    axes[1].annotate(f'{score:.2f}', xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()), xytext=(0, 3),\n",
    "                    textcoords=\"offset points\", ha='center', va='bottom')\n",
    "\n",
    "# Gráfico de barras da Precision\n",
    "bar3 = axes[2].bar(index, precision_scores, bar_width, label='Precision', alpha=0.7, color='r')\n",
    "axes[2].set_title('Precision', fontsize=12)\n",
    "axes[2].set_xticks(index)\n",
    "axes[2].set_xticklabels(models, rotation=45)\n",
    "axes[2].yaxis.grid(True, linestyle='--', alpha=0.6)\n",
    "axes[2].xaxis.grid(False)  # Desativar o grid vertical\n",
    "axes[2].set_ylim(0.0, 1.1)\n",
    "\n",
    "# Adicionar valores no topo das barras da Precision\n",
    "for bar, score in zip(bar3, precision_scores):\n",
    "    axes[2].annotate(f'{score:.2f}', xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()), xytext=(0, 3),\n",
    "                    textcoords=\"offset points\", ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Crie o DataFrame\n",
    "data = {\n",
    "    'Model': models,\n",
    "    'F1 Score': f1_scores,\n",
    "    'Recall': recall_scores,\n",
    "    'Precision': precision_scores,\n",
    "    'ROC AUC': roc_auc_scores,\n",
    "    'Training Time (s)': train_times,\n",
    "    'Prediction Time (s)': prediction_times,\n",
    "    'Total Time (s)': total_times\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Inclusão da coluna Total Time \n",
    "df['Total Time (s)'] = (df['Training Time (s)'] + df['Prediction Time (s)'])\n",
    "\n",
    "# Configure a exibição sem quebrar linhas\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9cc693-0c0c-442f-b521-a65cc7b78c1d",
   "metadata": {},
   "source": [
    "<h1>Experimento 1</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d939dee-bd25-4e14-a124-bf5c9623c10e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "################## Experimento 1 ##################\n",
    "\n",
    "import log_parsing\n",
    "import evaluation_metrics\n",
    "from log_parsing import load_BGL_data\n",
    "from data_preprocessing import sample_data\n",
    "\n",
    "##### Log BGL - TIME WINDOW #####\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    def generate_sequence(window_size, step_size, output_path):\n",
    "        config = {\n",
    "            \"window_size\": window_size, # Tamanho das sequências desejadas em fração de tempo ou número de eventos\n",
    "            \"step_size\": step_size, # Passo para avançar nas sequências em fração de tempo ou número de eventos\n",
    "            \"structured_file_path\": \"C:/Users/Alysson/Desktop/DissertacaoUNB/datasets/BGL/BGL_2k.log_structured.csv\",\n",
    "            \"BGL_sequence_path\": output_path\n",
    "        }\n",
    "        bgl_structured_data = load_BGL_data(config[\"structured_file_path\"], create_seconds_since=True, Label=True)\n",
    "        sequenceVector = sample_data(bgl_structured_data, config[\"window_size\"], config[\"step_size\"], config[\"BGL_sequence_path\"], Label=True, window_type='time')\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        for window_size, step_size in [(0.002778, 0.002778), (0.008333, 0.008333), (0.01667, 0.01667), (0.03333, 0.03333), (0.05, 0.05)]: #10seg, 30seg, 1min, 2min, 3min\n",
    "            output_path = f'C:/Users/Alysson/Desktop/DissertacaoUNB/datasets/BGL/BGL_2k-{window_size}horas.csv'\n",
    "            generate_sequence(window_size, step_size, output_path)\n",
    "            print(50*'-')\n",
    "            \n",
    "########## PROCESSAMENTO LOG BGL - LOTE ########## \n",
    "\n",
    "# Lista de valores de window size\n",
    "window_size_list = [0.002778, 0.008333, 0.01667, 0.03333, 0.05] #10seg, 30seg, 1min, 2min, 3min\n",
    "\n",
    "def create_train_test_data(hours, onlyNormalDataTrain=True, shuffleTrain=True, removeEmptySeq=True):\n",
    "    templatesTrain = pd.read_csv(r'C:\\Users\\Alysson\\Desktop\\DissertacaoUNB\\datasets\\BGL\\BGL_2k.log_templates.csv', sep=',')\n",
    "    SequenceVecTrain = pd.read_csv(r'C:\\Users\\Alysson\\Desktop\\DissertacaoUNB\\datasets\\BGL\\BGL_2k-{}horas.csv'.format(hours), sep=',') # time window\n",
    "    preProcess = {\"stemming\": False, \"lemmatization\": True}\n",
    "    \n",
    "    train_data, test_data, _ = create_train_test_dataset(templatesTrain, SequenceVecTrain, onlyNormalDataTrain=onlyNormalDataTrain, shuffleTrain=shuffleTrain, \n",
    "                                                      removeEmptySeq=removeEmptySeq, splitData=True, SequenceVectorTest=None, templatesTest=None, \n",
    "                                                      preProcessParam=preProcess)\n",
    "    return train_data, test_data\n",
    "\n",
    "# Itera sobre a lista de horas e cria os conjuntos de treinamento e teste\n",
    "train_data_list, test_data_list = [], []\n",
    "\n",
    "for window_size in window_size_list:\n",
    "    print('Time - Hours: ', window_size)\n",
    "    train_data, test_data = create_train_test_data(window_size)\n",
    "    train_data_list.append(train_data)\n",
    "    test_data_list.append(test_data)\n",
    "    print(50*'-')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e5e659-f6f2-4c3e-b73e-403b926b83cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "################## Experimento 1 ##################\n",
    "\n",
    "########## CRIAÇÃO DAS SEQUÊNCIAS DE VETORES SEMÂNTICOS - LOTE ########## \n",
    "word_vector_dict_train, dimension_train = generate_word_vectors_dict(train_data, model_type='glove50d')\n",
    "#word_vector_dict_test, dimension_test = generate_word_vectors_dict(test_data, model_type='glove50d')\n",
    "\n",
    "train_data_list_aux = []\n",
    "for train_data in train_data_list:\n",
    "    train_data['splittedReplacedSequence'] = train_data['ReplacedSequence'].apply(process_line)\n",
    "    train_data_aux = create_semantic_vector_sequence(train_data, word_vector_dict_train, dimension_train) \n",
    "    train_data_list_aux.append(train_data_aux)\n",
    "train_data_list = train_data_list_aux\n",
    "\n",
    "test_data_list_aux = []\n",
    "for test_data in test_data_list:\n",
    "    test_data['splittedReplacedSequence'] = test_data['ReplacedSequence'].apply(process_line)\n",
    "    test_data_aux = create_semantic_vector_sequence(test_data, word_vector_dict_train, dimension_train) \n",
    "    test_data_list_aux.append(test_data_aux)\n",
    "test_data_list = test_data_list_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a0e652-904a-4e0a-812d-ba6901ff8c50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "################## Experimento 1 - Treinamento e teste ##################\n",
    "\n",
    "# Lista de rótulos para as iterações\n",
    "iteration_labels = ['10 seg', '30 seg', '1 min', '2 min', '3 min']\n",
    "\n",
    "import pandas as pd  # Certifique-se de que você importou a biblioteca pandas\n",
    "\n",
    "# Detecção em lote\n",
    "def detect_anomalies(index, train_data, test_data, model='lof', label=True, plot_tsne=False, plot_metrics=False):\n",
    "    contamination = 0.15\n",
    "    \n",
    "    if model == 'lof':\n",
    "        params = {'algorithm': 'auto', 'contamination': contamination, 'leaf_size': 30, 'metric': 'minkowski', \n",
    "                  'n_jobs': None, 'n_neighbors': 20, 'novelty': True} # Default\n",
    "        _, df_anomalies, df_prediction, metricas, total_time = detect_lof_anomalies(train_data, test_data, metric_params=params, \n",
    "                                                                                    label=label, plot_tsne=plot_tsne, plot_metrics=plot_metrics)\n",
    "    elif model == 'autoencoder':\n",
    "        params = {'contamination': contamination}\n",
    "        _, df_anomalies, df_prediction, metricas, total_time = detect_autoencoder_anomalies(train_data, test_data, metric_params=params, \n",
    "                                                                                            label=label, plot_tsne=plot_tsne, plot_metrics=plot_metrics)\n",
    "    elif model == 'som':\n",
    "        params = {'contamination':contamination, 'grid_size':(10, 10), 'sigma':1.0, 'learning_rate':0.5}\n",
    "        _, df_anomalies, df_prediction, metricas, total_time = detect_som_anomalies(train_data, test_data, metric_params=params, \n",
    "                                                                                    label=label, plot_tsne=plot_tsne, plot_metrics=plot_metrics)\n",
    "    elif model == 'isolation_forest':\n",
    "        params = {'contamination': contamination, 'n_jobs': -1, 'random_state': 123} # Default\n",
    "        _, df_anomalies, df_prediction, metricas, total_time = detect_isolation_forest_anomalies(train_data, test_data, \n",
    "                                                                                                 metric_params=params, label=label, plot_tsne=plot_tsne, plot_metrics=plot_metrics)\n",
    "    elif model == 'mcd':\n",
    "        params = {'contamination':contamination, 'support_fraction': 1} # Default com support_fraction alterado para 1 para evitar os warnings\n",
    "        _, df_anomalies, df_prediction, metricas, total_time = detect_mcd_anomalies(train_data, test_data, \n",
    "                                                                                              metric_params=params, label=label, plot_tsne=plot_tsne, plot_metrics=plot_metrics)\n",
    "    elif model == 'hbos':\n",
    "        params = {'n_bins':10, 'alpha':0.15, 'tol':0.5, 'contamination':contamination} # Default\n",
    "        _, df_anomalies, df_prediction, metricas, total_time = detect_hbos_anomalies(train_data, test_data, metric_params=params, \n",
    "                                                                                     label=label, plot_tsne=plot_tsne, plot_metrics=plot_metrics)\n",
    "    elif model == 'cblof':\n",
    "        params= {'contamination':contamination} # Default\n",
    "        _, df_anomalies, df_prediction, metricas, total_time = detect_cblof_anomalies(train_data, test_data, metric_params=params, \n",
    "                                                                                      label=label, plot_tsne=plot_tsne, plot_metrics=plot_metrics)\n",
    "    elif model == 'gmm':\n",
    "        params = {'contamination': 0.1, 'n_components': 1, 'covariance_type': 'full', 'max_iter': 100, 'init_params': 'kmeans', 'random_state': 123} # Default\n",
    "        _, df_anomalies, df_prediction, metricas, total_time = detect_gmm_anomalies(train_data, test_data, metric_params=params, \n",
    "                                                                                    label=label, plot_tsne=plot_tsne, plot_metrics=plot_metrics)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model specified.\")\n",
    "\n",
    "    precision, recall, f1, fpr, tpr, roc_auc = metricas\n",
    "    train_time, prediction_time = total_time\n",
    "    return (df_anomalies, df_prediction, precision, recall, f1, fpr, tpr, roc_auc, train_time, prediction_time)\n",
    "\n",
    "# Criar listas vazias para cada métrica\n",
    "models = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "fpr_list = []\n",
    "tpr_list = []\n",
    "roc_auc_list = []\n",
    "train_time_list = []\n",
    "prediction_time_list = []\n",
    "\n",
    "# Criar listas vazias para cada métrica\n",
    "iteration_list = []  # Lista para armazenar a iteração atual\n",
    "\n",
    "# Semantic Vector List\n",
    "train_data_List = train_data_list\n",
    "test_data_List = test_data_list\n",
    "\n",
    "# Executar a detecção de anomalias para cada modelo e armazenar os resultados nas listas\n",
    "for i in range(len(iteration_labels)): # Número de iterações\n",
    "    lof_results = detect_anomalies(i, train_data_List[i], test_data_List[i], model='lof')\n",
    "    autoencoder_results = detect_anomalies(i, train_data_List[i], test_data_List[i], model='autoencoder')\n",
    "    som_results = detect_anomalies(i, train_data_List[i], test_data_List[i], model='som')\n",
    "    isolation_forest_results = detect_anomalies(i, train_data_List[i], test_data_List[i], model='isolation_forest')\n",
    "    mcd_results = detect_anomalies(i, train_data_List[i], test_data_List[i], model='mcd')\n",
    "    hbos_results = detect_anomalies(i, train_data_List[i], test_data_List[i], model='hbos')\n",
    "    cblof_results = detect_anomalies(i, train_data_List[i], test_data_List[i], model='cblof')\n",
    "    gmm_results = detect_anomalies(i, train_data_List[i], test_data_List[i], model='gmm')\n",
    "    \n",
    "    # Extrair as métricas de cada resultado e adicioná-las às listas\n",
    "    iteration_list.extend([i+1] * 8)  # Adiciona a iteração atual 9 vezes (uma para cada modelo)\n",
    "    models.extend(['LOF', 'AE', 'SOM', 'IF', 'MCD', 'HBOS', 'CBLOF', 'GMM'])\n",
    "    precision_list.extend([lof_results[2], autoencoder_results[2], som_results[2], isolation_forest_results[2], mcd_results[2], hbos_results[2], cblof_results[2], gmm_results[2]])\n",
    "    recall_list.extend([lof_results[3], autoencoder_results[3], som_results[3], isolation_forest_results[3], mcd_results[3], hbos_results[3], cblof_results[3], gmm_results[3]])\n",
    "    f1_list.extend([lof_results[4], autoencoder_results[4], som_results[4], isolation_forest_results[4], mcd_results[4], hbos_results[4], cblof_results[4], gmm_results[4]])\n",
    "    fpr_list.extend([lof_results[5], autoencoder_results[5], som_results[5], isolation_forest_results[5], mcd_results[5], hbos_results[5], cblof_results[5], gmm_results[5]])\n",
    "    tpr_list.extend([lof_results[6], autoencoder_results[6], som_results[6], isolation_forest_results[6], mcd_results[6], hbos_results[6], cblof_results[6], gmm_results[6]])\n",
    "    roc_auc_list.extend([lof_results[7], autoencoder_results[7], som_results[7], isolation_forest_results[7], mcd_results[7], hbos_results[7], cblof_results[7], gmm_results[7]])\n",
    "    train_time_list.extend([lof_results[8], autoencoder_results[8], som_results[8], isolation_forest_results[8], mcd_results[8], hbos_results[8], cblof_results[8], gmm_results[8]])\n",
    "    prediction_time_list.extend([lof_results[9], autoencoder_results[9], som_results[9], isolation_forest_results[9], mcd_results[9], hbos_results[9], cblof_results[9], gmm_results[9]])\n",
    "\n",
    "# Criar o DataFrame com as listas de métricas e iteração\n",
    "data = {\n",
    "    'Iteration': iteration_list,\n",
    "    'Model': models,\n",
    "    'Precision': precision_list,\n",
    "    'Recall': recall_list,\n",
    "    'F1 Score': f1_list,\n",
    "    'FPR': fpr_list,\n",
    "    'TPR': tpr_list,\n",
    "    'ROC AUC': roc_auc_list,\n",
    "    'Train Time': train_time_list,\n",
    "    'Prediction Time': prediction_time_list\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb17e03e-f59d-49f0-8af6-b03f400e41e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Definir diretório de salvamento\n",
    "save_directory = \"C:/Users/Alysson/Desktop/DissertacaoUNB/datasets/BGL/resultados\"\n",
    "\n",
    "# Definir configurações globais\n",
    "sns.set(style=\"whitegrid\")\n",
    "colors = ['blue', 'green', 'red', 'darkgreen', 'magenta', 'darkorange', 'black', 'purple']\n",
    "linestyles = ['-', '--', '-.', ':', '-', '-', '-', '-']\n",
    "marktypes = ['o', 's', '^', 'd', 'p', '*', 'x', 'v']\n",
    "iteration_labels = ['10 sec', '30 sec', '1 min', '2 min', '3 min']\n",
    "legend_labels = ['LOF', 'AE', 'SOM', 'IF', 'MCD', 'HBOS', 'CBLOF', 'GMM']\n",
    "metrics = ['F1 Score', 'Recall', 'Precision']\n",
    "\n",
    "# Função para gerar gráficos de linhas de métricas\n",
    "def plot_metric(df, metric, filename):\n",
    "    model_list = df['Model'].unique()\n",
    "    fig, ax = plt.subplots(figsize=(6, 7))\n",
    "    \n",
    "    for j, model in enumerate(model_list):\n",
    "        model_data = df[df['Model'] == model]\n",
    "        sns.lineplot(data=model_data, x='Iteration', y=metric, ax=ax, \n",
    "                     label=f'{model}', color=colors[j], linestyle=linestyles[j % len(linestyles)], \n",
    "                     marker=marktypes[j % len(marktypes)], markersize=10)\n",
    "    \n",
    "    ax.set_xticks(range(1, len(iteration_labels) + 1))\n",
    "    ax.set_xticklabels(iteration_labels, rotation=0, ha='center', fontsize=18)\n",
    "    ax.tick_params(axis='y', labelsize=18)\n",
    "    ax.set_xlabel('Window Size', fontsize=20)\n",
    "    ax.set_ylabel('', fontsize=20)\n",
    "    ax.set_ylim(0.0, 1.1)\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=True, ncol=4, fontsize='medium')\n",
    "    ax.grid(True, alpha=0.5)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "    # Salvar a figura no diretório especificado\n",
    "    plt.savefig(os.path.join(save_directory, filename), dpi=900, bbox_inches='tight', format='svg')\n",
    "    plt.close(fig)\n",
    "\n",
    "# Função para gerar gráficos ROC\n",
    "def plot_roc(df, iteration, filename):\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    iteration_df = df[df['Iteration'] == iteration]\n",
    "    \n",
    "    for j, model in enumerate(iteration_df['Model'].unique()):\n",
    "        model_df = iteration_df[iteration_df['Model'] == model]\n",
    "        fpr, tpr, roc_auc = model_df['FPR'].values[0], model_df['TPR'].values[0], model_df['ROC AUC'].values[0]\n",
    "        ax.plot(fpr, tpr, label=f'{legend_labels[j]} ({roc_auc:.2f})', \n",
    "                linestyle=linestyles[j], color=colors[j], linewidth=2.0, markersize=10)\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], linestyle='-', color='red', linewidth=0.5, label='Random (0.50)', markersize=10)\n",
    "    ax.set_xlabel('False Positive Rate (FPR)', fontsize=20)\n",
    "    ax.set_ylabel('True Positive Rate (TPR)', fontsize=20)\n",
    "    ax.tick_params(axis='x', labelsize=18)\n",
    "    ax.tick_params(axis='y', labelsize=18)\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.17), fancybox=True, shadow=True, ncol=2, fontsize='x-large')\n",
    "    ax.grid(True, alpha=0.5)\n",
    "\n",
    "    # Salvar a figura no diretório especificado\n",
    "    plt.savefig(os.path.join(save_directory, filename), dpi=900, bbox_inches='tight', format='svg')\n",
    "    plt.close(fig)\n",
    "\n",
    "# Função para gerar gráficos de tempos (Treinamento/Predição)\n",
    "def plot_times(df, time_column, filename):\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    for i, model in enumerate(df['Model'].unique()):\n",
    "        model_data = df[df['Model'] == model]\n",
    "        ax.plot(model_data['Iteration'], model_data[time_column], label=model, \n",
    "                linestyle=linestyles[i], color=colors[i], marker=marktypes[i], markersize=10)\n",
    "    \n",
    "    ax.set_xticks(df['Iteration'].unique())\n",
    "    ax.set_xticklabels(iteration_labels, fontsize=18)\n",
    "    ax.tick_params(axis='y', labelsize=18)\n",
    "    ax.set_xlabel('Window Size', fontsize=20)\n",
    "    ax.set_ylabel('Time(s)', fontsize=20)\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=True, ncol=4, fontsize='medium')\n",
    "    ax.grid(True, alpha=0.5)\n",
    "\n",
    "    # Salvar a figura no diretório especificado\n",
    "    plt.savefig(os.path.join(save_directory, filename), dpi=900, bbox_inches='tight', format='svg')\n",
    "    plt.close(fig)\n",
    "\n",
    "# Chamada das funções para salvar os gráficos no diretório especificado\n",
    "for metric in metrics:\n",
    "    plot_metric(df, metric, f'Experimento1-Metricas-{metric}.svg')\n",
    "\n",
    "for i, iteration in enumerate(df['Iteration'].unique()):\n",
    "    plot_roc(df, iteration, f'Experimento1-CurvasRoc-Subplot_{i}.svg')\n",
    "\n",
    "plot_times(df, 'Train Time', 'Experimento1-TempoTreinamento.svg')\n",
    "plot_times(df, 'Prediction Time', 'Experimento1-TempoPredicao.svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3f4a28-bf21-4e41-8626-f2b68bfbb915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b2c643c-0445-45ab-b2e1-0f2e2b591c36",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1>Experimento 2</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb06d79-4054-4b29-ad30-d6d24c37f6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Experimento 4 ##################\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Adicione o caminho do módulo ao sys.path\n",
    "module_path = r'C:\\Users\\Alysson\\Desktop\\DissertacaoUNB'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from create_train_test_dataset import create_train_test_dataset\n",
    "from semantic_vectorization import *\n",
    "import evaluation_metrics\n",
    "\n",
    "import data_preprocessing\n",
    "\n",
    "########## PROCESSAMENTO LOG BGL ########## \n",
    "templatesTrain = pd.read_csv(r'C:\\Users\\Alysson\\Desktop\\DissertacaoUNB\\datasets\\BGL\\BGL_2k.log_templates.csv', sep=',')\n",
    "SequenceVecTrain = pd.read_csv(r'C:\\Users\\Alysson\\Desktop\\DissertacaoUNB\\datasets\\BGL\\BGL_2k-0.01667horas.csv', sep=',')\n",
    "#SequenceVecTest = pd.read_csv(r'C:\\Users\\Alysson\\Desktop\\MinhaImplementacaoBGL\\datasets\\BGL\\XXXXXXX.csv', sep=',') #Opcional\n",
    "#templatesTest = pd.read_csv(r'C:\\Users\\Alysson\\Desktop\\MinhaImplementacaoBGL\\datasets\\BGL\\XXXXXXX.csv', sep=',') #Opcional\n",
    "preProcess = {\"stemming\": False, \"lemmatization\": True}\n",
    "\n",
    "train_data, test_data, _ = create_train_test_dataset(templatesTrain, SequenceVecTrain, onlyNormalDataTrain=True, shuffleTrain=True, removeEmptySeq=True, \n",
    "                                                  splitData=True, SequenceVectorTest=None, templatesTest=None, preProcessParam=preProcess)\n",
    "\n",
    "########## CRIAÇÃO DAS SEQUÊNCIAS DE VETORES SEMÂNTICOS - ÚNICA ########## \n",
    "\n",
    "# Use a função apply para aplicar a função process_line a cada linha da coluna 'ReplacedSequence'\n",
    "train_data['splittedReplacedSequence'] = train_data['ReplacedSequence'].apply(process_line)\n",
    "test_data['splittedReplacedSequence'] = test_data['ReplacedSequence'].apply(process_line) \n",
    "\n",
    "word_vector_dict_train, dimension_train = generate_word_vectors_dict(train_data, model_type='glove50d') \n",
    "#word_vector_dict_test, dimension_test = generate_word_vectors_dict(test_data, model_type='glove50d')\n",
    "\n",
    "train_data = create_semantic_vector_sequence(train_data, word_vector_dict_train, dimension_train)\n",
    "test_data = create_semantic_vector_sequence(test_data, word_vector_dict_train, dimension_train) \n",
    "\n",
    "print('Total train sequences: ' ,len(train_data))\n",
    "print('Total test sequences: ' ,len(test_data))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d7a73f-1174-4f68-b170-49ad461735c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from anomaly_detection import *\n",
    "\n",
    "# Lista de valores de contaminação\n",
    "contaminations = [0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "\n",
    "# Lista de funções de detecção de anomalias e seus parâmetros\n",
    "models = [\n",
    "    (\"LOF\", detect_lof_anomalies, {'algorithm': 'auto', 'contamination': None, 'leaf_size': 30, 'metric': 'minkowski', 'n_jobs': None, 'n_neighbors': 20, 'novelty': True}),\n",
    "    (\"Autoencoder\", detect_autoencoder_anomalies, {'contamination': None}),\n",
    "    (\"SOM\", detect_som_anomalies, {'contamination': None, 'grid_size': (10, 10), 'sigma': 1.0, 'learning_rate': 0.5}),\n",
    "    (\"Isolation Forest\", detect_isolation_forest_anomalies, {'contamination': None, 'max_features': 1.0, 'max_samples': 0.5, 'n_estimators': 100, 'n_jobs': -1, 'random_state': 123}),\n",
    "    (\"MCD\", detect_mcd_anomalies, {'contamination': None, 'support_fraction': 1}),\n",
    "    (\"HBOS\", detect_hbos_anomalies, {'n_bins': 10, 'alpha': 0.1, 'tol': 0.5, 'contamination': None}),\n",
    "    (\"CBLOF\", detect_cblof_anomalies, {'contamination': None}),\n",
    "    (\"GMM\", detect_gmm_anomalies, {'contamination': None, 'n_components': 1, 'covariance_type': 'full', 'max_iter': 100, 'init_params': 'kmeans', 'random_state': 123})\n",
    "]\n",
    "\n",
    "# Inicialização de dicionários para armazenar as métricas\n",
    "metrics = [\"recall\", \"precision\", \"f1\", \"roc_auc\"]\n",
    "results = {model[0]: {metric: {} for metric in metrics} for model in models}\n",
    "\n",
    "for contamination_value in contaminations:\n",
    "    for model_name, model_func, params in models:\n",
    "        params['contamination'] = contamination_value\n",
    "        _, _, _, model_metrics, _ = model_func(train_data, test_data, metric_params=params, label=True, plot_tsne=False, plot_metrics=False)\n",
    "        precision, recall, f1, _, _, roc_auc = model_metrics\n",
    "        \n",
    "        # Armazenar os resultados para cada métrica\n",
    "        results[model_name]['recall'][contamination_value] = recall\n",
    "        results[model_name]['precision'][contamination_value] = precision\n",
    "        results[model_name]['f1'][contamination_value] = f1\n",
    "        results[model_name]['roc_auc'][contamination_value] = roc_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0942ceb2-16cd-4708-9554-947c9ddd8195",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Lista de prefixos de modelos e os dicionários correspondentes\n",
    "model_prefixes = ['LOF', 'AE', 'SOM', 'IF', 'MCD', 'HBOS', 'CBLOF', 'GMM']\n",
    "metric_dicts = {\n",
    "    'F1': [f1_scores_lof, f1_scores_autoencoder, f1_scores_som, f1_scores_isolation_forest, f1_scores_mcd, f1_scores_hbos, f1_scores_cblof, f1_scores_gmm],\n",
    "    'Precision': [precision_scores_lof, precision_scores_autoencoder, precision_scores_som, precision_scores_isolation_forest, precision_scores_mcd, precision_scores_hbos, precision_scores_cblof, precision_scores_gmm],\n",
    "    'Recall': [recall_scores_lof, recall_scores_autoencoder, recall_scores_som, recall_scores_isolation_forest, recall_scores_mcd, recall_scores_hbos, recall_scores_cblof, recall_scores_gmm]\n",
    "}\n",
    "\n",
    "# Inicializar o dicionário com a coluna de contaminação\n",
    "metrics_data = {'Contamination': contaminations}\n",
    "\n",
    "# Preencher o dicionário dinamicamente para cada modelo e métrica\n",
    "for metric, dict_list in metric_dicts.items():\n",
    "    for prefix, metric_dict in zip(model_prefixes, dict_list):\n",
    "        metrics_data[f\"{prefix}_{metric}\"] = list(metric_dict.values())\n",
    "\n",
    "# Criar DataFrame\n",
    "df_metrics = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Exportar para CSV\n",
    "df_metrics.to_csv(r'C:/Users/Alysson/Desktop/DissertacaoUNB/datasets/BGL/resultados/Experimento2-Resultados.csv', index=None, header=True)\n",
    "\n",
    "# Exibir DataFrame\n",
    "df_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb533ddb-ebce-449d-87cc-ff9ac01d9473",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dicionários com as métricas e estilos de plotagem para cada modelo\n",
    "models = {\n",
    "    'LOF': {'f1': f1_scores_lof, 'precision': precision_scores_lof, 'recall': recall_scores_lof, 'marker': 'o', 'linestyle': '-', 'color': 'blue'},\n",
    "    'AE': {'f1': f1_scores_autoencoder, 'precision': precision_scores_autoencoder, 'recall': recall_scores_autoencoder, 'marker': 's', 'linestyle': '--', 'color': 'green'},\n",
    "    'SOM': {'f1': f1_scores_som, 'precision': precision_scores_som, 'recall': recall_scores_som, 'marker': '^', 'linestyle': '-.', 'color': 'red'},\n",
    "    'IF': {'f1': f1_scores_isolation_forest, 'precision': precision_scores_isolation_forest, 'recall': recall_scores_isolation_forest, 'marker': 'd', 'linestyle': ':', 'color': 'darkgreen'},\n",
    "    'MCD': {'f1': f1_scores_mcd, 'precision': precision_scores_mcd, 'recall': recall_scores_mcd, 'marker': 'p', 'linestyle': '-', 'color': 'magenta'},\n",
    "    'HBOS': {'f1': f1_scores_hbos, 'precision': precision_scores_hbos, 'recall': recall_scores_hbos, 'marker': '*', 'linestyle': '-', 'color': 'darkorange'},\n",
    "    'CBLOF': {'f1': f1_scores_cblof, 'precision': precision_scores_cblof, 'recall': recall_scores_cblof, 'marker': 'x', 'linestyle': '-', 'color': 'black'},\n",
    "    'GMM': {'f1': f1_scores_gmm, 'precision': precision_scores_gmm, 'recall': recall_scores_gmm, 'marker': 'v', 'linestyle': '-', 'color': 'purple'}\n",
    "}\n",
    "\n",
    "# Métricas a serem plotadas\n",
    "metric_titles = ['F1-Score', 'Precision', 'Recall']\n",
    "metric_keys = ['f1', 'precision', 'recall']\n",
    "\n",
    "# Criar subplots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 7))\n",
    "\n",
    "# Plotar as métricas\n",
    "for i, metric in enumerate(metric_keys):\n",
    "    for model, data in models.items():\n",
    "        axs[i].plot(contaminations, list(data[metric].values()), marker=data['marker'], linestyle=data['linestyle'], color=data['color'], label=model, markersize=10)\n",
    "    axs[i].set_xlabel('Contamination Rate')\n",
    "    axs[i].set_title(metric_titles[i])\n",
    "    axs[i].legend()\n",
    "    axs[i].set_ylim(0.0, 1.1)\n",
    "    axs[i].set_xticks(contaminations)\n",
    "    axs[i].set_xticklabels(contaminations, fontsize=12)\n",
    "    axs[i].grid(True, alpha=0.5)\n",
    "\n",
    "# Ajustar layout para evitar sobreposição e adicionar título geral\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "# Exportar figura\n",
    "plt.savefig('C:/Users/Alysson/Desktop/DissertacaoUNB/datasets/BGL/resultados/Experimento2-Metricas.svg', dpi=900, bbox_inches='tight', format='svg')\n",
    "\n",
    "# Mostrar figura\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87dc628-b5d8-4b90-8072-e28e6abcb404",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Experimento 2 - Resultados - subplots \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Função para salvar gráfico em uma figura separada\n",
    "def save_plot(x, y_lof, y_ae, y_som, y_if, y_mcd, y_hbos, y_cblof, y_gmm, title, filename):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "\n",
    "    plt.plot(x, y_lof, marker='o', linestyle='-', color='blue', label='LOF', markersize=10)\n",
    "    plt.plot(x, y_ae, marker='s', linestyle='--', color='green', label='AE', markersize=10)\n",
    "    plt.plot(x, y_som, marker='^', linestyle='-.', color='red', label='SOM', markersize=10)\n",
    "    plt.plot(x, y_if, marker='d', linestyle=':', color='darkgreen', label='IF', markersize=10)\n",
    "    plt.plot(x, y_mcd, marker='p', linestyle='-', color='magenta', label='MCD', markersize=10)\n",
    "    plt.plot(x, y_hbos, marker='*', linestyle='-', color='darkorange', label='HBOS', markersize=10)\n",
    "    plt.plot(x, y_cblof, marker='x', linestyle='-', color='black', label='CBLOF', markersize=10)\n",
    "    plt.plot(x, y_gmm, marker='v', linestyle='-', color='purple', label='GMM', markersize=10)\n",
    "\n",
    "    plt.ylim(0.0, 1.1)\n",
    "    plt.xticks(x)\n",
    "    plt.grid(True, alpha=0.5)\n",
    "    \n",
    "    # Ajustar o tamanho da fonte da label do eixo horizontal\n",
    "    #plt.xlabel('Taxa de Contaminação', fontsize=17)\n",
    "    plt.xlabel('Contamination Rate', fontsize=17)\n",
    "\n",
    "    # Ajustar o tamanho da fonte dos pontos dos eixos\n",
    "    plt.xticks(x, fontsize=14)\n",
    "    plt.yticks(fontsize=15)\n",
    "\n",
    "    #plt.title(title)\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.14), fancybox=True, shadow=True, ncol=4, fontsize='medium')\n",
    "\n",
    "    plt.savefig(filename, dpi=900, bbox_inches='tight', format='svg')\n",
    "    plt.close()\n",
    "\n",
    "# Gráfico (F1-Score)\n",
    "save_plot(contaminations, f1_scores_lof.values(), f1_scores_autoencoder.values(), f1_scores_som.values(),\n",
    "          f1_scores_isolation_forest.values(), f1_scores_mcd.values(), f1_scores_hbos.values(),\n",
    "          f1_scores_cblof.values(), f1_scores_gmm.values(), 'F1-Score', 'C:/Users/Alysson/Desktop/DissertacaoUNB/datasets/BGL/resultados/Experimento2-Metricas-F1Score.svg')\n",
    "\n",
    "# Gráfico (Precision)\n",
    "save_plot(contaminations, precision_scores_lof.values(), precision_scores_autoencoder.values(),\n",
    "          precision_scores_som.values(), precision_scores_isolation_forest.values(),\n",
    "          precision_scores_mcd.values(), precision_scores_hbos.values(),\n",
    "          precision_scores_cblof.values(), precision_scores_gmm.values(), 'Precision', 'C:/Users/Alysson/Desktop/DissertacaoUNB/datasets/BGL/resultados/Experimento2-Metricas-Precision.svg')\n",
    "\n",
    "# Gráfico (Recall)\n",
    "save_plot(contaminations, recall_scores_lof.values(), recall_scores_autoencoder.values(),\n",
    "          recall_scores_som.values(), recall_scores_isolation_forest.values(),\n",
    "          recall_scores_mcd.values(), recall_scores_hbos.values(),\n",
    "          recall_scores_cblof.values(), recall_scores_gmm.values(), 'Recall', 'C:/Users/Alysson/Desktop/DissertacaoUNB/datasets/BGL/resultados/Experimento2-Metricas-Recall.svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c445772-0ff9-4544-9274-b3611e2f6213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9734cfff-f599-4cdb-929c-4369f4d70c96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c517aa0-0427-4440-b6d6-d44dc5e45f80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dced6bc-2e2c-45fe-930d-3b5c57804da6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
